{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantasy Premier League team prediction\n",
    "### Intro\n",
    "\n",
    "Attempt to use some sort of AI to generate the optimal team, along with transfer and chip strategy eventually. Data from https://github.com/vaastav/Fantasy-Premier-League/.\n",
    "\n",
    "First cloning the dataset:\n",
    "```\n",
    "git clone https://github.com/vaastav/Fantasy-Premier-League.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pulp import LpMaximize, LpProblem, LpVariable, lpSum\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.opt import SolverFactory\n",
    "import chardet\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name  assists  attempted_passes  big_chances_created  \\\n",
      "0  Aaron_Cresswell        0                38                    0   \n",
      "1     Aaron_Lennon        0                12                    0   \n",
      "2       Aaron_Mooy        0                82                    0   \n",
      "3   Aaron_Ramsdale        0                 0                    0   \n",
      "4     Aaron_Ramsey        0                53                    0   \n",
      "\n",
      "   big_chances_missed  bonus  bps  clean_sheets  \\\n",
      "0                   0      0   22             1   \n",
      "1                   0      0    2             0   \n",
      "2                   0      0   14             1   \n",
      "3                   0      0    0             0   \n",
      "4                   0      0   12             0   \n",
      "\n",
      "   clearances_blocks_interceptions  completed_passes  ...  total_points  \\\n",
      "0                                8                28  ...             5   \n",
      "1                                1                 6  ...             1   \n",
      "2                                0                68  ...             3   \n",
      "3                                0                 0  ...             0   \n",
      "4                                1                50  ...             2   \n",
      "\n",
      "   transfers_balance  transfers_in  transfers_out  value  was_home  \\\n",
      "0              -1314           819           2133     50      True   \n",
      "1                 14           313            299     54     False   \n",
      "2             -15998          5326          21324     54      True   \n",
      "3                -82           194            276     40     False   \n",
      "4             187251        203836          16585     72     False   \n",
      "\n",
      "   winning_goals  yellow_cards  \\\n",
      "0              0             1   \n",
      "1              0             0   \n",
      "2              0             0   \n",
      "3              0             0   \n",
      "4              0             0   \n",
      "\n",
      "                                         source_file  GW  \n",
      "0  ../Fantasy-Premier-League/data/2017-18/gws/gw1... NaN  \n",
      "1  ../Fantasy-Premier-League/data/2017-18/gws/gw1... NaN  \n",
      "2  ../Fantasy-Premier-League/data/2017-18/gws/gw1... NaN  \n",
      "3  ../Fantasy-Premier-League/data/2017-18/gws/gw1... NaN  \n",
      "4  ../Fantasy-Premier-League/data/2017-18/gws/gw1... NaN  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "def find_gws_directory(base_directory):\n",
    "    # Walk through the base directory to find the 'gws' folder\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        if 'gws' in dirs:\n",
    "            return os.path.join(root, 'gws')\n",
    "    return None\n",
    "\n",
    "def load_and_bind_csvs(base_directory):\n",
    "    # Find the 'gws' directory\n",
    "    gws_directory = find_gws_directory(base_directory)\n",
    "    if not gws_directory:\n",
    "        print(\"Could not find 'gws' directory\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize an empty dictionary to hold dataframes by their columns set\n",
    "    dataframes_by_columns = {}\n",
    "\n",
    "    # Loop over all files in the 'gws' directory\n",
    "    for filename in os.listdir(gws_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(gws_directory, filename)\n",
    "            \n",
    "            # Detect encoding\n",
    "            with open(filepath, 'rb') as f:\n",
    "                result = chardet.detect(f.read())\n",
    "            encoding = result['encoding']\n",
    "            \n",
    "            # Read CSV file\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=encoding)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Add a column for the filepath including the name\n",
    "            df['source_file'] = filepath\n",
    "            \n",
    "            # Get a set of the columns\n",
    "            columns_set = frozenset(df.columns)\n",
    "            \n",
    "            # Bind similar CSVs together by their columns set\n",
    "            if columns_set in dataframes_by_columns:\n",
    "                dataframes_by_columns[columns_set].append(df)\n",
    "            else:\n",
    "                dataframes_by_columns[columns_set] = [df]\n",
    "    \n",
    "    # Concatenate dataframes with the same columns\n",
    "    final_dataframes = [pd.concat(dfs, ignore_index=True) for dfs in dataframes_by_columns.values()]\n",
    "    \n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "    final_df = pd.concat(final_dataframes, ignore_index=True, sort=False)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Specify the base directory containing the varying year folders\n",
    "base_directory = '../Fantasy-Premier-League/data'\n",
    "\n",
    "# Load and bind the CSV files\n",
    "final_df = load_and_bind_csvs(base_directory)\n",
    "\n",
    "# Display the head of the final dataframe\n",
    "if final_df is not None:\n",
    "    print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'assists', 'attempted_passes', 'big_chances_created',\n",
       "       'big_chances_missed', 'bonus', 'bps', 'clean_sheets',\n",
       "       'clearances_blocks_interceptions', 'completed_passes', 'creativity',\n",
       "       'dribbles', 'ea_index', 'element', 'errors_leading_to_goal',\n",
       "       'errors_leading_to_goal_attempt', 'fixture', 'fouls', 'goals_conceded',\n",
       "       'goals_scored', 'ict_index', 'id', 'influence', 'key_passes',\n",
       "       'kickoff_time', 'kickoff_time_formatted', 'loaned_in', 'loaned_out',\n",
       "       'minutes', 'offside', 'open_play_crosses', 'opponent_team', 'own_goals',\n",
       "       'penalties_conceded', 'penalties_missed', 'penalties_saved',\n",
       "       'recoveries', 'red_cards', 'round', 'saves', 'selected', 'tackled',\n",
       "       'tackles', 'target_missed', 'team_a_score', 'team_h_score', 'threat',\n",
       "       'total_points', 'transfers_balance', 'transfers_in', 'transfers_out',\n",
       "       'value', 'was_home', 'winning_goals', 'yellow_cards', 'source_file',\n",
       "       'GW'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
